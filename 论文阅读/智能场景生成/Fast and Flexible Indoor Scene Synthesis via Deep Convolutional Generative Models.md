# Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\1.PNG)



期刊：**Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition**，2019



## 1. Introduction

人们一生中大部分时间都在室内度过:在卧室、客厅、办公室、厨房，对这些现实空间的虚拟版本的需求从未如此之高。游戏、虚拟现实和增强现实体验经常发生在这样的环境中。最重要的一点，计算机视觉和机器人研究人员已经开始转向虚拟环境，以训练急需数据的模型，例如：场景理解和自主导航

本文模型的算法逻辑：在给定一个由建筑几何结构（地板、墙壁和天花板）划定的空室内空间时，该算法决定在该空间中放置哪些物体以及将它们放置在何处。而任何解决这一问题的模型都必须对==物体之间的存在性和空间关系==进行推理才能做出决策。

在计算机视觉中，用于此类推理的最灵活、最通用的机制是卷积，特别是通过用于图像理解的深度容量神经网络(CNNs)的形式实现的卷积。旧有的模型有着模型选择不恰当（过大的衣柜挡住了门）、生成时间过长（分钟为单位）等缺点。而基于图像的场景合成是很有前途的，因为它能够执行精确的、像素级的空间推理。

本文提出了一种基于深度卷积生成模型的`Image-based`场景合成管道，克服了以往工作中存在的问题。与之前的很多工作一样，==它通过迭代添加对象来生成场景==。然而，它将物体添加的步骤分解成不同的决策序列中：

- 全面考虑添加哪些物体
- 根据位置和方向对被添加的物体进行空间范围建模（model the spatial extent of objects to be added, in addition to their location and orientation.）

最后结果：合成一个场景平均需要不到2秒。

在定量实验和实际研究中，我们将我们的方法与先前的基于图像的方法、另一种先进的基于场景层次的深层生成模型以及人工创建的场景进行了比较。我们的方法比之前的技术表现得更好。



##  2. Related Work

***室内场景生成***：这一领域的一些最早的工作==利用室内设计原则和简单的统计关系==来安排预先指定的对象集；其他早期的工作尝试了==完全数据驱动==的场景合成，但由于训练数据和当时的学习方法有限，仅限于小规模的场景。

随着`SUNCG`等大型数据库的使用，新的数据驱动算法产生。我们的模型使用深度卷积生成模型来自动生成所有重要的对象属性——类别、位置、方向和大小。其它算法也使用了CNN：使用生成的对抗性网络以属性-矩阵形式生成场景（即每个场景对象一个列）；GRAINS使用递归神经网络对结构化的场景层次进行编码和采样；与我们的工作最相关的是[13]，它也使用深度卷积神经网络，该网络从上至下处理场景的图像表示，并通过顺序放置物体来合成场景。==我们的方法和他们的方法的主要区别是==

- 我们的方法用一个推理步骤对每个对象属性进行采样，而他们的方法执行数百个推理
- 我们的方法是根据类别、位置和方向形成的物体大小来产生distribution。我们的方法也使用单独的模块来预测类别和位置，从而避免了他们的方法所显示的一些失败案例。



***深层生成模型（Deep Generative Models）***：除了分析数据分布之外，深度神经网络正在被越来越多地用于构建强大的模型，而我们的模型正是利用了这种能力。深度潜变量模型`Deep latent variable models`，特别是变量自动编码器（`variational autoencoders：VAEs`）和生成式对抗网络（`generative adversarial networks：GANs`），因其能够将看似任意的数据分布打包到表现良好的低维 "潜空间 `latent spaces`"中而受到欢迎。

我们使用自回归模型来生成室内场景，逐个对象进行构建，其中每一步都以迄今为止生成的场景为条件。

> 自回归生成模型已被用于对图像中的对象进行无监督解析，用顺序视觉注意生成自然图像，解析手绘图的图像，通过基元的顺序组合生成3D对象，以及控制程序化图形程序的输出，以及其他应用。



***训练数据***

最近的一些研究表明，室内场景下的模型可以通过训练大量的虚拟室内场景合成的图像来改进。



## 3. Model

为了最大化灵活性，我们使用了一个连续的生成模型，每次迭代地插入一个对象，直到完成为止。除了从一个空房间生成完整的场景外，==这种范式自然也支持从部分场景的生成==——只需用一个部分填充的场景来初始化进程。

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\2.PNG)

它首先提取一个自上而下的，地板平面图表示的输入场景，然后将其传入由四个决策模块组成的序列，用来决定如何选择和添加对象到场景中。这些模块决定在场景中添加什么类别的物体，该物体应该位于哪里，它应该面向什么方向，以及它的物理尺寸。

### 3.1 Next Object Category

管道的第一个模块的目标是，给定一个自顶向下的场景图像表示，预测要添加到场景中的对象的类别。模块需要判断哪些对象已经存在、有多少对象以及房间中的可用空间。为了让模型也能决定何时停止，我们在类别集中增加了一个额外的`<STOP>`类别。模块使用`Resnet18`对场景图像进行编码，它还提取场景中所有类别物体的数量，并将其编码为一个全连接层。最后，模型将这两个编码连接起来，然后输入到另一个FC，输出类别的概率分布。在测试时，模块从预先选定的分布中抽取样本以选择下一个类别。

<img src="C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\3.PNG" style="zoom:67%;" />

> 从一个空场景开始，下一个类别分布是由一个或两个大的，经常出现的物体(例如床和衣柜，用于卧室场景)主导。其他类别的概率随着场景开始填充而增加，直到场景变得足够填充并且“<STOP>”类别开始占主导地位。

在基于图像的场景合成中，以往的研究==共同预测了种类和位置==。这导致了一个缺点，正如作者所指出的那样，很可能出现在某一地点的物体会被重复（即错误地）取样，例如将多个床头柜放在床的左边。相比之下，我们的类别预测模块对场景进行全局推理，从而避免了这个问题。



### 3.2 Object Location

在这个模块中，我们的模型使用输入场景和预测类别来决定场景中该类别的位置。我们把这个问题看作是一个图像到图像的转换问题：给定自顶向下的场景图像，输出一个“热图”`heatmap`图像，包含一个物体在那里出现的每个像素的概率。这种表示是有利的，因为它可以被视为一个（潜在的高度多模态）二维离散分布，我们可以抽样产生一个新的位置

这种像素级的离散分布与之前的工作类似，只是他们将分布逐像素组装，对场景的每个像素调用一次深度卷积网络。相比之下，我们的模块使用单次正向通过全卷积编码器-解码器网络（FCN）来一次性预测整个分布。

This module uses a `Resnet34 encoder` followed by an `up-convolutional decoder`。解码器输出一个$64\times 64\times|C|$的图片，C是种类数。然后，该模块切出与感兴趣的类别对应的通道，并通过重新归一化将其视为二维概率分布。我们怀疑，训练同一个网络来预测所有类别，可以为网络提供更多关于不同位置的上下文，例如，它不应该只是学习预测某个位置的衣柜，它还可以学习这是因为床头柜更可能出现在那里。

之后，超出房间范围的无题概率置为0；预测第二级类别(例如台灯)的位置时，它还将数据集中未作为该类别的支持表面的概率归零。

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\4-1602056563488.PNG)

> 床和衣柜的分布应避免在可能堵塞门的地方放置。床头柜的分布是双峰的，每个都紧紧地集中在床头周围

为了训练网络，我们使用`pixel-wise`交叉熵损失。与之前的工作一样，我们在类别中增加了一个 `empty space`的类别，它允许网络推理出对象除了应该在哪里之外，还不应该在哪里。在训练损失计算中，这种像素的权重是被占用像素的10倍。

由于每个训练实例的地面真值标签是一个单一的位置，而不是一个分布，我们的模型有可能==过度拟合到那个精确的位置==。这在图a与c中显示，预测的分布折叠成单点位置。网络很可能会尝试将输入房间与几个记忆的房间进行匹配，其中没有一个是合理的。为了处理这个问题，我们通过应用L2正则化和dropout来削弱网络的容量，迫使它学习一个结构上相似场景接近的潜空间。这导致了平均输出位置，即位置的连续分布。

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\5.PNG)

在进入下一个模块之前，我们的系统转换输入的场景图像，使其以预测位置为中心。



### 3.3 Object Orientation

输入依然是俯视图和预测的物体种类，我们假设每个类别都有一个规范的正面方向。我们不是预测旋转角度，而是预测前向方向矢量$[sin\theta,cos\theta]$（必须归一化）。我们的模块预测cos θ以及一个给定sin θ符号的布尔值。 这里，我们发现每个类别使用单独的网络权重是最有效的。

这组可能的朝向有可能是多种模式的：例如，房间角落里的一张床可以背靠角落的任何一面墙。为了对这种行为建模，我们使用conditional variational auto encoder（条件 变分自编码器）`CVAE`来实现它。

具体来说，我们使用CNN对输入场景进行编码，然后将其与从多变量单位正态分布中采样的潜伏码`latent code z`进行串联，然后送到一个FC的解码器，以产生cos θ和sin θ的符号。在训练时，我们使用标准的`CVAE`损失公式（即有一个额外的编码器网络）来学习一个近似的后验分布。

由于室内场景经常被直线建筑所包围，其中的物体经常精确地对准枢轴方向。然而，`CVAE`是一个概率模型，采样的方向是有噪声的。为了让我们的模块能够在适当的时候产生精确的对齐，这个模块包括第二个CNN，它接收输入场景并预测要插入的对象是否应该将其预测的方向 "啪 "的一声拍到最近的四个枢轴方向上。

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\6.PNG)

> 床头柜的朝向是单一的，高度受限于它与床和墙壁的关系。台灯通常是对称的，这就导致了多模式下的预测方向分布。把扶手椅放在房间的角落是最自然的对角方向的角落，但一些变化是可能的

在进入下一个模块之前，我们的系统场景图像由预测的角度进行旋转。



### 3.4 Object Dimensions

给定一个场景图像转化为某一特定对象类别的局部坐标框架，尺寸模块预测对象的空间范围。也就是说，它为要插入的对象预测了一个对象空间边界框。这也是一个多模态的问题，甚至比方位问题更严重（比如很多长度不一的衣柜可以靠在同一面墙上）。同样，我们使用`CVAE`来解决这个问题：CNN对场景进行编码，用z进行连通，然后使用全连接的解码器来产生边界框的[x，y]尺寸。

==人眼对尺寸上的误差非常敏感==，比如一个物体太大，从而穿透了旁边的墙壁。为了帮助微调预测结果，我们还在`CVAE`训练中加入了一个对抗性损失项。这个损失使用卷积判别器，它将输入场景与预测边界框的符号距离场（SDF）进行通道并联。与方向模块一样，该模块也是按类别使用单独的网络权重。

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\7.PNG)

> 图7可视化了不同对象放置场景的预测大小分布。预测的分布捕获了不同物体的可能尺寸范围，例如电视架的长度可以有很大的变化。然而，在如图7所示的情况下，在床头柜和墙壁之间放置了一个垫子，由于这个高度受限的位置，预测的分布是低方差的。



### 3.5 Object Insertion

为了给定预测的类别、位置、方向和尺寸，==选择一个特定的3D模型来插入，我们通过我们的数据集进行最近邻搜索==，以找到与预测的对象尺寸紧密匹配的3D模型。当存在多个可能的候选模型时，我们更倾向于那些在数据集中与房间中已有的其他对象频繁共存的模型。偶尔，插入的对象会与房间中现有的对象发生冲突。在这种情况下，我们选择另一个相同类别的对象。

在极少数情况下（小于1%），不存在可能的插入物。如果出现这种情况，我们从预测的类别分布中重新取样一个不同的类别，然后再试一次。



## 4. Data & Training

我们使用SUNCG数据集来训练我们的模型，SUNCG数据集是一个在线室内设计工具[28]的用户设计的4万多个场景的集合。在本文中，我们将实验的重点放在四种常见的房间类型上：卧室、客厅、浴室和办公室。我们从SUNCG中提取这些类型的房间，进行预处理以过滤掉不常见的对象类型、错误标注的房间等。经过预处理，我们得到了6300间卧室（40个对象类别）、1400间起居室（35个类别）、6800间浴室（22个类别）、1200间办公室（36个类别）。

==为了给我们所有的模块生成训练数据，我们遵循同样的一般程序==：从我们的数据集中抽取一个场景，从其中移除一些对象子集，然后让模块预测要添加的 "下一个 "对象（即被移除的对象之一）。这个过程需要对每个场景中的对象进行排序。我们使用简单的几何启发式方法来推断对象之间的静态支持关系（例如桌子支持的灯），并且我们保证所有支持的对象按照这个顺序排在其支持的父对象之后：我们首先保证所有这样的受支持的 "二级 "对象都在所有 "一级 "对象（即由地板支持的对象）之后。对于类别预测模块，我们进一步根据对象的重要性进行排序，我们将其定义为类别的平均大小乘以其在数据集中的出现频率

这样做要求场景中的对象有一个稳定的、规范的排序；如果没有这样的排序，我们发现每一步都有太多有效的可能类别，我们的模型很难在多个对象中建立连贯的场景。对于所有其他模块，我们使用随机化的排序方式。最后，对于位置模块，FCN的任务不是预测单个下一个物体的位置，而是预测从训练场景中移除的所有缺失物体的位置，这些物体的支撑面存在于部分场景中。

我们针对不同的房间类别分别训练管道中的每个模块。经验上，我们发现类别模块在300，000个训练例子后表现最好，而位置模块在1，000，000个例子后表现最好。由于方向和维度模型所解决的问题更多的是局部性的，所以它们的行为在不同的时间段内更稳定。在实践中，使用方位模块训练了2，000，000个例子，维度模块训练了1，000，000个例子。



## 5. Results & Evaluation

==Scene completion==：图8展示了部分场景完成的例子，其中我们的模型以一个不完整的场景作为输入，并建议多个next对象来填充场景。我们的模型样品不同的完整模型,同样的开始部分场景。这个例子还强调了我们的模型处理非矩形房间(最下面一行)的能力，

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\8.PNG)

==Object category distribution==：

场景生成模型要想很好地捕获训练数据，一个必要的条件是其合成结果中出现的对象类别分布应该与训练集的分布非常相似。方法：Kullback- Leibler divergence D~KL~(P~synth~||P~dataset~) 

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\T1.PNG)



==Scene classification accuracy==

见论文

==Effectiveness of our design choices==

见论文



==Speed comparisons==：表3显示了不同方法合成一个完整场景所花费的时间。我们的模型在NVIDIA Geforce GTX 1080Ti GPU上生成一个完整的场景平均需要不到2秒的时间，这比之前的基于图像的方法(Deep Priors)快了两个量级。虽然比[16]等端到端方法慢，但我们的模型还可以执行场景完成和下一个对象建议等任务，==这两种特点在实时应用中都很有用==。

![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\T3.PNG)

==Perceptual study==：

参与者会看到两张自上而下并排渲染的场景图像，并被要求选择他们认为更可信的那一张。每个物体类别的图像都使用纯色渲染，以排除任何材料或纹理外观的影响。对于每个比较和每个房间类型，我们招募了10名参与者，这足以产生强大的95%置信区间。每个参与者进行了55次比较；其中5次是 "警惕性测试"，对随机乱七八糟的场景进行比较，以检查参与者是否注意。我们过滤掉没有通过所有警惕性测试的参与者。

表4显示了本研究的结果。在所有房间类型中，==我们生成的场景明显优于GRAINS生成的场景==（GRAINS不提供浴室或办公室结果）。由于格式上的差异，我们对GRAINs房间几何形状的重建是不完美的。我们手动删除了物体与墙壁相交的房间，但应该注意的是，重建的房间可能仍然与他们工作中提出的结果略有不同。==与Deep Priors方法相比，我们的场景对于卧室和浴室是首选，对于起居室则被判定为无差别==。然而，我==们生成的办公室场景却不太受欢迎==。我们推测，==这是因为办公室的训练数据是高度多模态的，包含个人办公室、团体办公室、会议室等==。在我们看来，Deep Priors方法生成的房间大多是个人办公室。我们也一直在生成高质量的个人办公室。但是，当类别模块尝试对其他类型的办公室进行抽样调查时，这个意图并没有很好地传达给其他模块，导致结果无序化，比如一张小桌子，十把椅子。最后，==与SUNCG中憋出来的人类创造的场景相比，我们的结果对于卧室和浴室来说是无法区分的，对于客厅来说几乎是无法区分的，而对于办公室来说同样是不太喜欢的。==


![](C:\Users\Cooler\Desktop\JMX\ShaderToy\论文阅读\智能场景生成\Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models.assets\T4.PNG)



## 6. Conclusion

见论文