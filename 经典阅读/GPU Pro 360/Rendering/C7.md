# Rasterized Voxel-Based Dynamic Global Illumination

![image-20210824120520814](C7.assets/image-20210824120520814.png)



## 1. 介绍

**光栅化的、基于体素的动态全局照明**考虑到场景，尤其是室内，可能包含大量**不同类型的光源**（例如，点光源、聚光灯和定向灯）。同时，它在视觉上产生了良好和稳定的结果，同时保持了**较高的交互帧率**。



## 2. 前瞻

这项技术有效地利用了`DirectX 11`硬件所引入的功能，以实现上述结果。

在第一步中，通过使用硬件光栅器，为场景创建了**一个体素网格表示**。在这里，场景在**没有深度测试**的情况下被渲染到一个**小的二维渲染目标**中，但不是将结果输出到绑定的渲染目标中，而是通过使用**原子函数**将栅格化的像素写入一个**三维读写缓冲器**中。这样一来，**光栅化器就变成了一个 "体素化器"**，有效地创建了一个高度填充的场景的三维网格表示。这个体素网格包含了**几何体的漫反射和法线信息**，随后被用来**生成间接光照**和测试几何体遮挡的情况。由于**每一帧都会重新创建体素网格**，因此所提出的技术是完全动态的，**不依赖任何预计算**。

在第二步，**网格内的体素**被每个光源照亮。然后，照明被转换为**虚拟点光源**（`VPLs`），以**二阶球面谐波系数**的形式存储。图形硬件通过使用**内置的混合阶段**再次得到利用，以便将**每个光源的结果**结合起来。随后，**生成的VPLs**在网格内传播，以产生间接照明。与[Kaplanyan和Dachsbacher 10]提出的`light propagation volume`（LPV）技术相比，既不需要为每个光源创建**反射阴影图**，也不需要事后将**VPLs**注入网格中。



## 3. 实现

这个技术主要可分为五个步骤。

### 创建场景的体素网格表示

我们首先需要定义**立体体素网格的属性**，即它的`extents`、位置和**视图/投影矩阵**。网格与观察者的摄像机**同步移动**，并与**网格单元的边界**永久绑定，以**避免**由于场景的离散体素网格表示而**出现闪烁**。为了正确地将我们的场景映射到体素网格，我们需要使用==正投影==；因此，我们将使用**三个视图矩阵**来获得**更高的场景覆盖率**：一个矩阵用于从后到前的视图，一个矩阵用于从右到左的视图，还有一个用于从上到下的视图。所有其他的计算将完全在`GPU`上完成。

接下来我们渲染**位于网格边界内的场景几何图形**，禁用颜色写入，不进行深度测试，将其渲染到**一个小型的2D渲染目标**中。我们将使用一个$32×32×32$ 的网格；为此，完全可以使用一个$64×64$像素的渲染目标，并使用**最小的可用像素格式**，因为无论如何我们都会将结果输出到一个**读写缓冲区**。基本上，我们通过**顶点着色器**将三角形顶点传递给**几何着色器**。在几何着色器中，我们选择了**三角形最明显的视图矩阵**，以便为基元实现最高数量的光栅化像素。此外，**归一化设备坐标中的三角形大小**被当前绑定的渲染目标的`texel`大小所增加。这样一来，由于当前绑定的渲染目标的分辨率较低而被丢弃的像素仍然会被栅格化。**栅格化的像素**被原子化地写入像素着色器中的**3D读写结构缓冲区**。这样一来，就不需要在几何着色器中**放大几何图形**，以获得场景的**高填充三维网格表示**。

[list 1](Generation of the voxel grid)

```c++
// vertex shader
VS_OUTPUT main(VS_INPUT input) 
{
    VS_OUTPUT output; 
    output.position = float4(input.position, 1.0f); 
    output.texCoords = input.texCoords; 
    output.normal = input.normal; 
    return output;
} 

// geometry shader
static float3 viewDirections[3] = {
    float3(0.0f, 0.0f, -1.0f), // back to front 
    float3(-1.0f, 0.0f, 0.0f), // right to left 
    float3(0.0f, -1.0f, 0.0f) // top to down
};

int GetViewIndex(in float3 normal) 
{
    float3x3 directionMatrix; 
    directionMatrix[0] = -viewDirections[0]; 
    directionMatrix[1] = -viewDirections[1]; 
    directionMatrix[2] = -viewDirections[2]; 
    
    float3 dotProducts = abs(mul(directionMatrix, normal)); 
    float maximum = max(max(dotProducts.x, dotProducts.y), dotProducts.z); 
    
    int index; 
    if(maximum == dotProducts.x) 
        index = 0;
    else if(maximum == dotProducts.y) 
        index = 1;
    else 
        index = 2;
    return index; 
}

[maxvertexcount(3)] 
void main(triangle VS_OUTPUT input[3], inout TriangleStream<GS_OUTPUT> outputStream)
{
    float3 faceNormal = normalize(input[0].normal + input[1].normal+ input[2].normal);
    
    // Get view , at which the current triangle is most visible , in order to 
    // achieve highest possible rasterization of the primitive. 
    int viewIndex = GetViewIndex(faceNormal);
    
    GS_OUTPUT output[3]; 
    [unroll] 
    for(int i = 0;i < 3; i++) 
    {
        output[i].position = mul(constBuffer.gridViewProjMatrices[viewIndex], 
                                 input[i].position);
        output[i].positionWS = input[i].position.xyz; // world -space position 
        output[i].texCoords = input[i].texCoords; 
        output[i].normal = input[i].normal;
    }
    
    // Increase size of triangle in normalized device coordinates by the 
    // texel size of the currently bound render -target. 
    float2 side0N = normalize(output[1].position.xy - output[0].position.xy); 
    float2 side1N = normalize(output[2].position.xy - output[1].position.xy); 
    float2 side2N = normalize(output[0].position.xy - output[2].position.xy); 
    float texelSize = 1.0f / 64.0f; 
    output[0].position.xy += normalize(-side0N + side2N) * texelSize; 
    output[1].position.xy += normalize(side0N - side1N) * texelSize; 
    output[2].position.xy += normalize(side1N - side2N) * texelSize;
    
    [unroll]
     for(int j = 0;j < 3; j++) 
     	outputStream.Append(output[j]);
    
     outputStream.RestartStrip();
}

// pixel shader 
struct VOXEL {
    uint colorMask; // encoded color 
    uint4 normalMasks; // encoded normals 
    uint occlusion; // voxel only contains geometry info if occlusion > 0
}; 

RWStructuredBuffer<VOXEL> gridBuffer : register(u1);

// normalized directions of four faces of a tetrahedron 
static float3 faceVectors[4] = 
{
    float3(0.0f, -0.57735026f , 0.81649661f), 
    float3(0.0f, -0.57735026f , -0.81649661f), 
    float3(-0.81649661f, 0.57735026f, 0.0f), 
    float3(0.81649661f, 0.57735026f, 0.0f)
};

int GetNormalIndex(in float3 normal, out float dotProduct) 
{
    float4x3 faceMatrix; 
    faceMatrix[0] = faceVectors[0]; 
    faceMatrix[1] = faceVectors[1]; 
    faceMatrix[2] = faceVectors[2]; 
    faceMatrix[3] = faceVectors[3]; 
    float4 dotProducts = mul(faceMatrix, normal); 
    float maximum = max(max(dotProducts.x, dotProducts.y), max(dotProducts.z, dotProducts.w));
    
    int index; 
    if(maximum == dotProducts.x) 
        index = 0;
    else if(maximum == dotProducts.y) 
        index = 1;
    else if(maximum == dotProducts.z) 
        index = 2;
    else 
        index = 3;
    
    dotProduct = dotProducts[index]; 
    return index;
}

void main(GS_OUTPUT input) {
    float3 base = colorMap.Sample(colorMapSampler, input.texCoords).rgb;
    
    // Encode color into the lower 24 bit of an unsigned integer , using 
    // 8 bit for each color channel. 
    uint colorMask = EncodeColor(base.rgb);
    
    // Calculate color -channel contrast of color and write value into the 
    // highest 8 bit of the color mask. 
    float contrast = length(base.rrg - base.gbb)/ (sqrt(2.0f) + base.r + base.g + base.b);
    int iContrast = int(contrast * 255.0f); 
    colorMask |= iContrast << 24;
    
    // Encode normal into the lower 27 bit of an unsigned integer , using 
    // for each axis 8 bit for the value and 1 bit for the sign. 
    float3 normal = normalize(input.normal); 
    uint normalMask = EncodeNormal(normal.xyz);
    
    // Calculate to which face of a tetrahedron current normal is closest 
    // and write corresponding dot product into the highest 5 bit of the 
    // normal mask. 
    float dotProduct; 
    int normalIndex = GetNormalIndex(normal, dotProduct); 
    int iDotProduct = int(saturate(dotProduct) * 31.0f); 
    normalMask |= iDotProduct << 27;
    
    // Get offset into voxel grid. 
    float3 offset = (input.positionWS - constBuffer.snappedGridCenter) * constBuffer.invGridCellSize;
    offset = round(offset);
    
    // Get position in voxel grid. 
    int3 voxelPos = int3(16 ,16 ,16) + int3(offset);
    
    // Only output voxels that are inside the boundaries of the grid. 
    if((voxelPos.x > -1) && (voxelPos.x < 32) && (voxelPos.y > -1) 
       && (voxelPos.y < 32) && (voxelPos.z > -1) && (voxelPos.z < 32))
    {
        // Get index into voxel grid. 
        int gridIndex = (voxelPos.z * 1024) + (voxelPos.y * 32) + voxelPos.x;
        
        // Output color. 
        InterlockedMax(gridBuffer[gridIndex].colorMask, colorMask);
        
        // Output normal according to normal index. 
        InterlockedMax(gridBuffer[gridIndex].normalMasks[normalIndex], normalMask);
        
        // Mark voxel that contains geometry information.
        InterlockedMax(gridBuffer[gridIndex].occlusion, 1);
    }
}
```

由于体素是**实际场景的简化表示**，详细的几何信息会丢失。为了在最终的全局照明输出中放大`color bleeding`，在其颜色通道中**具有高差异的颜色**（"对比度"）是首选。通过将对比度值写入**整数颜色掩码的最高8位**，具有最高对比度的颜色将**自动占主导地位**，因为我们用`InterlockedMax()`将结果写入**体素网格**中。由于，例如，**薄的几何体**在一个单一的体素中可以有**相反的法线**，所以不仅是颜色，而且法线也必须被仔细地写入体素中。因此，要确定当前法线与四面体的哪个面最接近。通过将**相应的点积**写入**整数法线掩码的最高5位**，自动选择离每个四面体面最近的法线，因为我们再次用`InterlockedMax()`写入结果。根据检索到的四面体面，法线被写入**体素的相应法线通道**中。之后，当体素被照亮时，会选择**离光矢量最近的法线**，这样就可以获得最佳的照明效果。

![image-20210825095615247](C7.assets/image-20210825095615247.png)

由于所有的操作都是使用一个**非常小的渲染目标**（$64×64$像素）进行的，所以这个**生成步骤的速度**快得令人吃惊。上图显示了一张截图，其中**Sponza场景的体素网格表示**被可视化了。为了覆盖整个场景，我们使用了**两个嵌套的体素网格**：一个细分辨率网格用于**靠近观察者的区域**，一个粗分辨率网格用于**远处的区域**。



### 在体素空间创建VPLs

在这一步，我们完全从**先前生成的体素网格**中创建`VPLs`。对于位于**网格边界内的每个光源**，我们渲染一个$32×32$像素的**小四边形**。通过使用硬件实例化，对于每个四边形，我们能够通过**一次绘图调用**来渲染**32个实例**。在通过**顶点着色器**传递顶点后，**几何着色器**将在当前绑定的**2D纹理数组**中选择相应的渲染目标片断。然后，像素着色器将**根据当前光源的类型**，照亮所有包含几何信息的体素。最后，**被照亮的体素**被转换为**VPLs的二阶球面谐波表示**。通过使用**加法硬件混合**，所有光源的结果被自动合并。

[list 2](VPL creation)

```c++
// vertex shader

VS_OUTPUT main(VS_INPUT input, uint instanceID : SV_InstanceID) 
{
    VS_OUTPUT output; 
    output.position = float4(input.position, 1.0f); 
    output.instanceID = instanceID; 
    return output ;
} 

// geometry shader 

struct GS_OUTPUT {
    float4 position : SV_POSITION; 
    uint rtIndex : SV_RenderTargetArrayIndex;
};

[maxvertexcount (4)] 
void main(line VS_OUTPUT input[2], inout TriangleStream<GS_OUTPUT> outputStream)
{
    // Generate a quad from two corner vertices. 
    GS_OUTPUT output[4];
    
    // lower -left vertex 
    output [0].position = float4(input[0].position.x, input[0].position.y, input[0].position.z, 1.0f);
    
    // lower -right vertex
    output[1].position = float4(input[1].position.x, input[0].position.y, input[0].position.z, 1.0f);
    
    // upper -left vertex 
    output[2].position = float4(input[0].position.x, input[1].position.y, input[0].position.z, 1.0f);

    // upper -right vertex 
    output[3].position = float4(input[1].position.x, input[1].position.y, input[0].position.z, 1.0f);
    
    // By using hardware instancing , the geometry shader will be invoked 32 
    // times with the corresponding instance ID. For each invocation the 
    // instance ID is used to determine into which slice of a 2D texture 
    // array the current quad should be rasterized. 
    [unroll] 
    for(int i = 0;i < 4; i++) 
    {
        output[i].rtIndex = input[0].instanceID; 
        outputStream.Append(output[i]); 
    } 
    
    outputStream.RestartStrip();
}

// pixel shader 
StructuredBuffer<VOXEL> gridBuffer : register(t0);

struct FS_OUTPUT 
{
    float4 fragColor0 : SV_TARGET0; // red SH -coefficients 
    float4 fragColor1 : SV_TARGET1; // blue SH -coefficients 
    float4 fragColor2 : SV_TARGET2; // green SH -coefficients
}; 

// Calculate second -order SH -coefficients for clamped cosine lobe function. 
float4 ClampedCosineSHCoeffs(in float3 dir) 
{
    float4 coeffs; 
    coeffs.x = PI / (2.0f * sqrt(PI)); 
    coeffs.y = -((2.0f * PI) / 3.0f) * sqrt(3.0f / (4.0f * PI)); 
    coeffs.z = ((2.0f * PI ) / 3.0f) * sqrt (3.0f / (4.0f * PI)); 
    coeffs.w = -((2.0f * PI)/3.0f) * sqrt (3.0f / (4.0f * PI)); 
    coeffs.wyz *= dir;
    return coeffs;
} 

// Determine which of the four specified normals is closest to the 
// specified direction. The function returns the closest normal and as 
// output parameter the corresponding dot product. 
float3 GetClosestNormal(in uint4 normalMasks, in float3 direction, out float dotProduct)
{
    float4x3 normalMatrix; 
    normalMatrix[0] = DecodeNormal(normalMasks.x); 
    normalMatrix[1] = DecodeNormal(normalMasks.y);
    normalMatrix[2] = DecodeNormal(normalMasks.z); 
    normalMatrix[3] = DecodeNormal(normalMasks.w); 
    float4 dotProducts = mul(normalMatrix, direction);
    
    float maximum = max(max(dotProducts.x, dotProducts.y), max(dotProducts.z, dotProducts.w));

    int index; 
    if(maximum == dotProducts.x) index = 0;
    else if(maximum == dotProducts.y) index = 1;
    else if(maximum == dotProducts.z) index = 2;
    else index = 3;
    
    dotProduct = dotProducts[index]; 
    return normalMatrix[index];
}


PS_OUTPUT main(GS_OUTPUT input) 
{
    PS_OUTPUT output;
    
    // Get index of current voxel. 
    int3 voxelPos = int3(input.position.xy, input.rtIndex); 
    int gridIndex = (voxelPos.z * 1024) + (voxelPos.y * 32) + voxelPos.x;
    
    // Get voxel data and early out , if voxel has no geometry information. 
    VOXEL voxel = gridBuffer[gridIndex]; 
    if(voxel.occlusion == 0) 
        discard;
    
    // Get world -space position of voxel. 
    int3 offset = samplePos - int3(16, 16, 16); 
    float3 position = (float3(offset) * constBuffer.gridCellSize) + constBuffer.snappedGridCenter;
    
    
    // Decode color of voxel. 
    float3 albedo = DecodeColor(voxel.colorMask);
    
    // Get normal of voxel that is closest to the light direction. 
    float nDotL; 
    float3 normal = GetClosestNormal(voxel.normalMasks, lightVecN, nDotL);
    
    // Calculate diffuse lighting according to current light type.
    float3 diffuse = CalcDiffuseLighting(albedo, nDotL);
    
#ifdef USE_SHADOWS
    
    // Calculate shadow term according to current light type with the help 
    // of a shadow map. 
    float shadowTerm = ComputeShadowTerm(position); 
    diffuse *= shadowTerm;
#endif
    
    // Calculate clamped cosine lobe SH -coefficients for VPL. 
    float4 coeffs = ClampedCosineSHCoeffs(normal);
    
    // Output SH -coefficients for each color channel. 
    output.fragColor0 = coeffs * diffues.r; 
    output.fragColor1 = coeffs * diffuse.g; 
    output.fragColor2 = coeffs * diffuse.b;
    return output;
}
```

为了输出**三个颜色通道的二阶SH系数**，我们渲染成三个二维纹理数组，精度为半浮点。由于所有的计算都是在**体素空间**中完成的，并且仅限于体素，而体素实际上包含了几何信息，所以这种技术在各种不同类型的光源数量不断增加的情况下可以很好地扩展。在许多情况下，我们甚至可以放弃使用**点光源和聚光灯的阴影贴图**，而不会对最终的渲染输出产生明显的影响。然而，对于大型的点光源、聚光灯和定向灯，我们确实需要使用**阴影贴图**来避免漏光。在这里，我们可以简单地重复使用在**直接照明步骤中已经创建的阴影贴图**。



### Propagate VPLs

//todo



### Apply Indirect Lighting

//todo



## 4. 结果

![image-20210825101649768](C7.assets/image-20210825101649768.png)





# Reducing Texture Memory Usage by 2-Channel Color Encoding

![](C7.assets/scene-files-abelcine_v1.detail.jpg)



## 1. 介绍

**单一材料的纹理**往往没有表现出很大的色彩变化，只包含**有限的色调范围**，而使用材料表面内的高光和暗光（如阴影）区域所产生的全部亮度范围。这里介绍的方法旨在将**任何给定的纹理**编码为两个通道：一个通道保留**完整的亮度信息**，另一个通道专门用于**色调/饱和度编码**。



## 2. 纹理压缩算法

![image-20210825102456416](C7.assets/image-20210825102456416.png)

上图展示了著名的**RGB色彩空间**，它被描绘成一个**单位立方体**。每个源`texel`的颜色对应于这个立方体中的一个点。用**两个通道**来逼近这个空间，实际上意味着我们必须在这个单位立方体中找到**一个嵌入的表面**（二维流形），它尽可能地接近**源纹理的`texels`集**。为了简化解码算法，我们可以使用一个**简单的平面**，或者严格地说，**一个平面与RGB单元立方体的交点**（上图右）。因为我们已经决定，**亮度信息**应该在一个单独的通道中进行**无损编码**，彩色平面应该通过**RGB空间的零亮度原点**（黑色）。因此，==双通道压缩的简化色彩空间是由一个单一的三维向量——平面法线定义的==。



### 颜色平面估计

**拟合一个平面**来近似一组三维点是一项常见的任务，存在各种算法。为了找到**颜色简化的最佳平面**，我们必须采取以下准备步骤。

首先，我们必须记住，**大多数图像文件格式**中的RGB像素颜色值并**不代表线性基色贡献**。为了本算法的目的，我们要在**线性RGB色彩空间**中操作。大多数常见的文件格式都提供**sRGB空间的数值**。虽然内部比较复杂，但这种表示方法可以用**伽马校正**（`2.2`）来转换到**线性空间**。严格来说，我们将在伽马值为`1.1`的RGB空间中操作。虽然这种轻微的非线性对**估计和编码**只有很小的影响，但在解码后转换回**sRGB空间**时，必须使用**相同的简化伽马值**`2`，以避免亮度水平的变化。

在将颜色值转换为**线性RGB空间**后，我们必须记住的另一件事是，**色相感知是RGB成分之间关系的结果，而不是线性的**。为了尽可能正确地匹配色调，我们最好使用**感知上的线性色彩空间**。然而，这将导致**更昂贵的解码阶段**，因此我们将限制自己使用**线性RGB颜色空间**，接受**潜在的轻微色调错误**。尽管如此，为了尽量减少**不在感知上正确的线性RGB空间中操作**的影响，我们可以在**估计平面**之前对空间应用**非均匀缩放**。这将影响**整个RGB通道的误差分布**，允许一些色调，以**牺牲其他色调**为代价，被更紧密地表现出来。这种非均匀缩放的结果是，随着RGB成分的缩小，它们对颜色平面的影响也会缩小，因为沿着缩小的轴线的距离会缩短。由于**色调感知的非线性**，为所有潜在的纹理**一次性定义缩放系数并不容易**，在我们的测试中，它们是根据**样本纹理集**实验性地设置的。首先，我们尝试了**在亮度计算中使用的RGB分量权重**（把最重要的放在`G`上，几乎没有放在`B`上），但实验表明，当用**更平衡的权重**进行估计时，一些材料的纹理得到了更好的体现。为了对各种纹理取得可接受的结果，我们使用了实验中选择的权重集，即红色为$1/2$，绿色为$1$，蓝色为$1/4$。

通过上述两个操作，整个**初始像素颜色处理**可以表示为：

![image-20210825103646105](C7.assets/image-20210825103646105.png)

考虑到上述因素，每个`texel`的颜色代表三维空间中的一个点。**最佳的近似颜色平面**将是使该平面与每个点之间的**平方距离之和最小的平面**。因为该平面被假设为经过点$(0,0,0)$，我们可以用**它的法线**来表示。实际上，**点-平面距离**的计算简化为**点积**。注意，由于我们使用的是RGB空间，矢量分量被标记为`r`、`g`和`b`，而不是通常的x、y和z。

![image-20210825104115786](C7.assets/image-20210825104115786.png)

**最佳平面法线矢量**是指最小化**点--平面距离**的矢量。 这类问题可以用**最小平方拟合方法**来解决，该方法旨在最小化平方距离之和。我们想要**最小化的近似误差**表示为：

![image-20210825104208511](C7.assets/image-20210825104208511.png)

经过**简单的变换**后，变成了：

![image-20210825104227579](C7.assets/image-20210825104227579.png)

对于最小化的实现，我们可以使用上述方程来计算**六个部分的和**。然后，我们可以使用**蛮力方法**来测试一组**预定义的潜在法线矢量**，以找到**总近似误差最小的那一个**。因为每次测试都是在**线性时间**内进行的，只需要花费几次乘法和加法，所以这种方法的速度还是可以接受的。

找到最佳颜色平面后的**最后一步**是恢复由颜色分量引起的**颜色空间失真**。因为平面法线是一个表面法线向量，法线的非统一空间缩放的通常规则适用，我们必须用**法线**乘以要使用的**矩阵的反转置**。虽然转置并不影响缩放矩阵，但**矩阵的反转**却影响了缩放，最后的缩放操作是使用非互换权重。

![image-20210825104610395](C7.assets/image-20210825104610395.png)

由于所有后续的计算通常都是在**线性RGB空间**中完成的，因此我们不必转换为**sRGB**。



### 计算基颜色

**编码和解码过程的重要参数是两种基色**。切过**RGB单位立方体**的颜色平面形成一个**三角形或四边形**，其中一个角位于点$(0,0,0)$。这个形状中与点$(0,0,0)$**相邻的两个角**被定义为**平面色彩空间的基色**，如下图所示。平面上**所有其他可用的颜色**都位于点$(0,0,0)$和**两个基色点**所形成的角度内。因为**颜色平面**从$(0,0,0)$开始并进入单位立方体，所以**基色点**总是位于**单位立方体的轮廓**上，从点$(0,0,0)$看去。为了找到基色，我们可以简单地计算**与剪影边缘的平面交点**，从而得到所需的一对点。我们必须记住，平面可以划过剪影顶点，甚至嵌入一对剪影边缘。因此，为了计算这些点，我们可以使用一种算法，围绕剪影行走，计算剪影穿过平面的两个点。

现在的关键观察是，我们可以把==色相值==表示为**与跨越平面的向量的角度**，使用两个基色之间的线性插值。为了计算出最终的颜色，我们只需要**调整亮度**，并进行任何所需的最终色彩空间转换。

![image-20210825104914035](C7.assets/image-20210825104914035.png)



### 亮度编码

**被编码的颜色的亮度**被直接存储。在颜色被转换到**线性RGB空间**后，我们可以使用一个经典的方程式来获得从sRGB到XYZ颜色空间转换中得出的**感知亮度值**：

![image-20210825105304700](C7.assets/image-20210825105304700.png)

由于亮度感知不是线性的，我们使用**2的伽玛值**来存储亮度。这与标准的伽马值`2.2`很接近，代价是高光部分的质量会明显下降。另外，伽马值为`2`意味着：亮度在编码时可以简单地计算其平方根，而在解码时将简单地进行平方。



### Hue计算和编码

![image-20210825105713391](C7.assets/image-20210825105713391.png)

为了对**颜色的色调**进行编码，我们必须在近似平面上找到**最接近的合适的颜色**，然后找到**混合基色的比例**以获得适当的色调。上图展示了色相编码过程，可以概述如下。

1. 将**线性RGB空间中的颜色点**投影到颜色平面上。
2. 计算平面上该点的**二维坐标**。
3. 在平面上找到一条通过(0,0,0)和该点的**二维线**。
4. 找出该线与**基色点之间的二维线**的交叉比例，即确定**基色的混合系数**。

第一步是一个**简单的几何运算**。从第二步开始。我们必须对**嵌入平面内的二维坐标**进行几何运算。有了两个基色点`A`和`B`，我们可以计算平面的二维坐标为

![image-20210825105914958](C7.assets/image-20210825105914958.png)

然后用**点乘法**计算平面内任何一点的二维坐标。

![image-20210825105943746](C7.assets/image-20210825105943746.png)

由于原点和投射到平面上的点都有**相同的二维坐标**，我们可以**完全跳过上述算法中的第1步**。当考虑到嵌入在颜色平面内的点时，**计算色调的基色混合系数的问题**现在被简化为两条线的交叉问题：一条连接两个基色点的线和一条通过原点和被编码平面上的点的线。这给了我们以下的线-线相交方程：

![image-20210825110315706](C7.assets/image-20210825110315706.png)

解决这个线性方程的`t`，我们就得到了结果。然后，**这个混合系数**被简单地直接存储在**第二通道**中，完成了**双通道的编码过程**。



## 3. 解码算法

解码算法很简单，见下列清单。首先，基色`bc1`和`bc2`作为**常量数据**被传递，并与来自**第二通道数据的混合因子**混合，从而得到一个具有所需色调的颜色，但亮度不对。这个亮度被计算为`color_lum`。接下来，我们计算出所需的亮度`target_lum`，作为第一通道数据的平方值（因为我们用伽马`2`存储了亮度）。由于得到的颜色处于线性色彩空间，我们可以通过**简单地用当前亮度除以所需亮度，然后乘以颜色来调整亮度**。如果需要，我们当然可以将计算出来的颜色转换为非线性色彩空间，以达到演示的目的。

[list 1]()

```c++
float3 texture_decode(float2 data, float3 bc1, float3 bc2) 
{
    float3 color = lerp(bc1, bc2, data.y); 
    float color_lum = dot(color, float3 (0.2126, 0.7152, 0.0722)); 
    float target_lum = data.x * data.x;
    
    color *= target_lum / color_lum; 
    return color;
}
```



## 4. 编码图像质量

![image-20210825110907283](C7.assets/image-20210825110907283.png)

![image-20210825110917058](C7.assets/image-20210825110917058.png)





# Particle-Based Simulation of Material Aging

![image-20210825111112130](C7.assets/image-20210825111112130.png)

## 1. 介绍

==老化的外观==，是游戏和电影制作中内容艺术家的一项日常任务。非常需要可交互引导的自动模拟，以支持由**交互物体引起的老化效果**的合理性，例如，铁锈的滴落。本章描述了GPU辅助的材料老化模拟，它是基于**可定制的老化规则**和通过颗粒的**材质传输**。



## 2. 前瞻
